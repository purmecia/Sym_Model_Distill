# Symbolic Knowledge Distillation

We explore the design
space and limitations of the student model, including various sizes, architectures,
and data-augmentation methods in order to determine the optimal design for cap-turing commonsense knowledge. In addition, we train a critic model to automat-ically score the output of the student model, and to serve as a heuristic for beam search in order to improve performance without the need for a larger model. The effectiveness of the proposed method is demonstrated through experimentation
and examples of rejected output sentences.

Detailed Report can be found [here](https://drive.google.com/file/d/1c1nrO8gNJijDpsEuln4bZqALjriu60w7/view?usp=sharing).

## Data

- training data: data used to finetune the student model
- labeled data: human-labeled data for evaluating student models and training critic models
- generated_result: sentences generated by student models

## Codes

- symbolic-knowledge-distillation: originial code provided by based paper
- generate & generate-2020: generate dataset to finetune student models
- Student_model: example of finetuning student model
- text_labeling: helper for labling generated sentences by hand
- critic_model: example of finetuning critic_model
- critic_inference: combine critic model with student model to allow student model generate results by beam search

## Usage

1. Finetune student model with Student_model.ipynb and dataset in training_data
2. Label generated senteces with text_labeling.ipynb
3. Finetune critic model with labled dataset (exp: critic_labeled_data.tsv in labeled data folder) using critic_model.ipynb
4. Generate beam search - based student model with critic_inference.ipynb